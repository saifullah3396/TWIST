{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f69240334f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The main script that serves as the entry-point for all kinds of training experiments.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import dataclasses\n",
    "import gc\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from das.data.data_args import DataArguments\n",
    "from das.utils.arg_parser import DASArgumentParser\n",
    "from das.utils.basic_args import BasicArguments\n",
    "from das.utils.basic_utils import configure_logger, create_logger\n",
    "\n",
    "# setup logging\n",
    "logger = create_logger(__name__)\n",
    "\n",
    "# define dataclasses to parse arguments from\n",
    "ARG_DATA_CLASSES = [BasicArguments, DataArguments]\n",
    "\n",
    "# torch hub bug fix https://github.com/pytorch/vision/issues/4156\n",
    "torch.hub._validate_not_a_forked_repo = lambda a, b, c: True\n",
    "\n",
    "\n",
    "def parse_args(cfg):\n",
    "    \"\"\"\n",
    "    Parses script arguments.\n",
    "    \"\"\"\n",
    "    # initialize the argument parsers\n",
    "    arg_parser = DASArgumentParser(ARG_DATA_CLASSES)\n",
    "\n",
    "    return arg_parser.parse_yaml_file(os.path.abspath(cfg))\n",
    "\n",
    "def print_args(title, args):\n",
    "    \"\"\"\n",
    "    Pretty prints the arguments.\n",
    "    \"\"\"\n",
    "    args_message = f\"\\n{title}:\\n\"\n",
    "    for (k, v) in dataclasses.asdict(args).items():\n",
    "        args_message += f\"\\t{k}: {v}\\n\"\n",
    "    print(args_message)\n",
    "\n",
    "\n",
    "def empty_cache():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\"\"\"\n",
    "Initializes the training of a model given dataset, and their configurations.\n",
    "\"\"\"\n",
    "\n",
    "# empty cuda cache\n",
    "empty_cache()\n",
    "\n",
    "# parse arguments\n",
    "cfg = 'dataset.yaml'\n",
    "basic_args, data_args = parse_args(cfg)\n",
    "\n",
    "# configure pytorch-lightning logger\n",
    "pl_logger = logging.getLogger(\"pytorch_lightning\")\n",
    "configure_logger(pl_logger)\n",
    "\n",
    "# intialize torch random seed\n",
    "torch.manual_seed(basic_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-16 11:16:51 port-41xx das.data.data_modules.base[29061] INFO Preparing / preprocesing dataset and saving to cache...\n",
      "2021-11-16 11:16:51 port-41xx das.data.data_modules.base[29061] INFO Setting up train/validation dataset...\n",
      "2021-11-16 11:16:51 port-41xx das.data.data_modules.base[29061] INFO Training stage == None\n",
      "2021-11-16 11:16:51 port-41xx das.data.data_modules.base[29061] INFO Setting up train/validation dataset...\n",
      "2021-11-16 11:16:51 port-41xx das.data.datasets.base[29061] INFO Loading dataset [rvlcdip-train] from cache directory: //netscratch/saifullah/document_analysis_stack/datasets/rvlcdip/train\n",
      "2021-11-16 11:16:51 port-41xx das.data.datasets.base[29061] INFO Defining data transformations [train]:\n",
      "2021-11-16 11:16:51 port-41xx das.data.datasets.base[29061] INFO Loading dataset [rvlcdip-val] from cache directory: //netscratch/saifullah/document_analysis_stack/datasets/rvlcdip/val\n",
      "2021-11-16 11:16:51 port-41xx das.data.datasets.base[29061] INFO Defining data transformations [val]:\n",
      "2021-11-16 11:16:51 port-41xx das.data.data_modules.base[29061] INFO Training set size = 319756\n",
      "2021-11-16 11:16:51 port-41xx das.data.data_modules.base[29061] INFO Validation set size = 40000\n",
      "2021-11-16 11:16:51 port-41xx das.data.data_modules.base[29061] INFO Setting up test dataset...\n",
      "2021-11-16 11:16:51 port-41xx das.data.datasets.base[29061] INFO Loading dataset [rvlcdip-test] from cache directory: //netscratch/saifullah/document_analysis_stack/datasets/rvlcdip/test\n",
      "2021-11-16 11:16:51 port-41xx das.data.datasets.base[29061] INFO Defining data transformations [test]:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ConvertImageDtype()\n",
      "\t ConvertImageDtype()\n",
      "\t ConvertImageDtype()\n"
     ]
    }
   ],
   "source": [
    "from das.data.data_modules.base import DataModuleFactory\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import ocrodeg \n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "# initialize data-handling module, set collate_fns later\n",
    "datamodule = DataModuleFactory.create_datamodule(\n",
    "    basic_args, data_args)\n",
    "\n",
    "# prepare the modules\n",
    "datamodule.prepare_data()\n",
    "datamodule.setup()\n",
    "\n",
    "def random_transform_fn(image, params):\n",
    "    return ocrodeg.transform_image(image, **ocrodeg.random_transform())\n",
    "\n",
    "def random_distortion_fn(image, params):\n",
    "    noise = ocrodeg.bounded_gaussian_noise(image.shape, params['sigma'], 5.0)\n",
    "    return ocrodeg.distort_with_noise(image, noise)\n",
    "\n",
    "def ruled_surface_distortion_fn(image, params):\n",
    "    noise = ocrodeg.noise_distort1d(image.shape, magnitude=params['noise'])\n",
    "    return ocrodeg.distort_with_noise(image, noise)\n",
    "\n",
    "def gaussian_fn(image, params):\n",
    "    return ndi.gaussian_filter(image, params['str'])\n",
    "\n",
    "def threshold_fn(image, params):\n",
    "    blurred = ndi.gaussian_filter(image, params['str'])\n",
    "    return 1.0*(blurred>0.5)\n",
    "\n",
    "def binary_blur_fn(image, params):\n",
    "    return ocrodeg.binary_blur(image, params['str'])\n",
    "\n",
    "def noisy_binary_blur_fn(image, params):\n",
    "    return ocrodeg.binary_blur(image, 0.1, noise=params['noise'])\n",
    "\n",
    "def random_blotches_fn(image, params=None):\n",
    "    return ocrodeg.random_blotches(image, 3e-4, 1e-4)\n",
    "\n",
    "def fibrous_noise_fn(image, params=None):\n",
    "    return ocrodeg.printlike_fibrous(image)\n",
    "    # noise = ocrodeg.make_fibrous_image((256, 256), 700, 300, 0.01)\n",
    "\n",
    "def multiscale_noise_fn(image, params=None):\n",
    "    return ocrodeg.printlike_multiscale(image)\n",
    "\n",
    "distortions = {\n",
    "    'random_transform': (random_transform_fn, [{}]),\n",
    "    'random_distortion': (random_distortion_fn, [{'sigma': 1.0}, {'sigma': 2.0}, {'sigma': 5.0}, {'sigma': 10.0}, {'sigma': 20.0}]),\n",
    "    'ruled_surface_distortion': (ruled_surface_distortion_fn, [{'noise': 5.0}, {'noise': 20.0}, {'noise': 100.0}, {'noise': 150.0}, {'noise': 200.0}]),\n",
    "    'gaussian': (gaussian_fn, [{'str': 1.0}, {'str': 2.0}, {'str': 3.0}, {'str': 4.0}, {'str': 5.0}]),\n",
    "    'threshold': (threshold_fn, [{'str': 0.1}, {'str': 0.25}, {'str': 0.5}, {'str': 0.75}, {'str': 1.0}]),\n",
    "    'binary_blur': (binary_blur_fn, [{'str': 0.1}, {'str': 0.5}, {'str': 1.0}, {'str': 1.5}, {'str': 2.0}]),\n",
    "    'noisy_binary_blur': (noisy_binary_blur_fn, [{'noise': 0.01}, {'noise': 0.05}, {'noise': 0.1}, {'noise': 0.2}, {'noise': 0.3}]),\n",
    "    'random_blotches': (random_blotches_fn, [{}]),\n",
    "    'fibrous_noise': (fibrous_noise_fn, [{}]),\n",
    "    'multiscale_noise': (multiscale_noise_fn, [{}]),\n",
    "}\n",
    "\n",
    "# load the data\n",
    "output_dir = Path(\"/netscratch/saifullah/rvl-cdip-wo-tobacco3842-c/\")\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "\n",
    "# s\n",
    "for idx in range(1): #len(datamodule.train_dataset)):\n",
    "    sample = datamodule.train_dataset[idx]\n",
    "    output_image_path = output_dir / sample['image_file_path'].split('images/')[1]\n",
    "    if not output_image_path.parent.exists():\n",
    "        output_image_path.parent.mkdir(parents=True)\n",
    "    plt.imsave(str(output_image_path.with_suffix(''))+\"_orig.jpg\", sample['image'].squeeze(), cmap='gray')\n",
    "    for distortion, distortion_cfg in distortions.items():\n",
    "        distortion_fn = distortion_cfg[0]\n",
    "        distortion_params = distortion_cfg[1]\n",
    "\n",
    "        for param in distortion_params:\n",
    "            output_image_path_dist = Path(output_image_path.with_suffix(\"\") / distortion).with_suffix('')\n",
    "            if not output_image_path_dist.exists():\n",
    "                output_image_path_dist.mkdir(parents=True)\n",
    "\n",
    "            params_string = ''\n",
    "            for k, v, in param.items():\n",
    "                params_string += f'{k}={v}_' \n",
    "\n",
    "            if params_string == '':\n",
    "                params_string = 'params=fixed'\n",
    "\n",
    "            params_string += '.jpg'\n",
    "\n",
    "            distorted_image = distortion_fn(sample['image'].squeeze().cpu().numpy(), param)\n",
    "            plt.imsave(output_image_path_dist / params_string, distorted_image, cmap='gray')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cc3803ff4c70635b3705e3d6b639c0de7f0fe170b9184eb396efc73b74d73b8d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('nlp': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
